{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "# from public_tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e8c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Preprocessing of data############\n",
    "\n",
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "# Example of a picture\n",
    "index = 22  # train_set_x_orig > is saved as a np array with the size (209,64,64,3); which the first is the number of images, the second and the third are the dimensions of an image and the last is 3 changels of RGB\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n",
    "\n",
    "# Finding the values for number of training examples, number of test examples, and dimensions of a training image\n",
    "m_train=train_set_x_orig.shape[0]\n",
    "m_test=test_set_x_orig.shape[0]\n",
    "num_px=train_set_x_orig.shape[1]\n",
    "# now we need to reshape the data so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\\_px $*$ num\\_px $*$ 3, 1). \n",
    "# One way to do above (flattern matrix X of shape (a,b,c,d) to a matrix X_flattern of shape (b*c*d,a)) is to use: X_flatten = X.reshape(X.shape[0], -1).T \n",
    "train_set_x_flatten=train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "test_set_x_flatten=test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "# now we need to standardize our dataset:\n",
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc54a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Bulding different functions for our model########\n",
    "\n",
    "#### Sigmoid Func\n",
    "def sigmoid(z):\n",
    "    s=1/(1+np.exp(-z))\n",
    "    return s\n",
    "### Initialize Func\n",
    "def initialize_with_zeros(dim):\n",
    "    w=np.zeros((dim,1))\n",
    "    b=0.0\n",
    "    return w,b\n",
    "## Propagate\n",
    "def propagate(w,b,X,Y):\n",
    "    m=X.shape[1]\n",
    "    #forward propagation\n",
    "    A=sigmoid(np.dot(w.T,X)+b)\n",
    "    test=np.dot(Y,np.log(A).T)+np.dot((1-Y),np.log(1-A).T)\n",
    "    cost=(-1/m)*(test)   # be careful that here sum is inside matrix multiply\n",
    "    #backward propagation \n",
    "    dw=(1/m)*(np.dot(X,(A-Y).T))\n",
    "    db=(1/m)*(np.sum(A-Y))\n",
    "    cost=np.squeeze(np.array(cost))   \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "### Optimze Func to learn w and b by minimizing the cost function J (using Gradient Discent algorithm)\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        w=w-learning_rate*dw\n",
    "        b=b-learning_rate*db\n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "### Predic Func to predit based on learned w and b\n",
    "def predict(w,b,X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1) # w has the same size as feautures (number of pixels of an image)\n",
    "    A=sigmoid(np.dot(w.T,X)+b)\n",
    "    for i in range(A.shape[1]): #A.shape[1] is the number of images that we want to preedict\n",
    "        if A[0,i]>0.5:\n",
    "            Y_prediction[0,i]=1\n",
    "        else:\n",
    "            Y_prediction[0,i]=0\n",
    "    return Y_prediction\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc57637",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Now it's timen to merge all the functions together to build a model##############\n",
    "### Model Func\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    dim=X_train.shape[0]\n",
    "    w, b = initialize_with_zeros(dim)\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=False)\n",
    "    w=params[\"w\"]\n",
    "    b=params[\"b\"]\n",
    "    Y_prediction_train=predict(w,b,X_train)\n",
    "    Y_prediction_test=predict(w,b,X_test)\n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here is how we can implement the model\n",
    "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot learning curve (with costs)\n",
    "costs = np.squeeze(logistic_regression_model['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
