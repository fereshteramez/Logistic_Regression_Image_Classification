{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9a9836",
   "metadata": {},
   "source": [
    "## Logistic Regression|\n",
    "*We will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n",
    "\n",
    "1-First we need to import the necessary libraries\\\n",
    "2- We need 3 sets; training set (a set of images labeled as target(for example if cat, y=1 otherwise y=0)0, test set and data set.\\\n",
    "3- we should find the values for:\n",
    "- m_train (number of training examples)\n",
    "- m_test (number of test examples)\n",
    "- num_px (= height = width of a training image)\n",
    "\n",
    "Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].\n",
    "\n",
    "4-Then convert the image of size image.shape=(a,b,c,d) to image.reshape(b*d*c,a) (convert an image to a vector)\\\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use:\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "5- Standardize our data set \n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\\\n",
    "train_set_x = train_set_x_flatten / 255.\n",
    "\n",
    "**cmomon steps for pre-processing a new dataset are**\\\n",
    "    1. Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\\\n",
    "    2. Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)\\\n",
    "    3. Standardize the data\n",
    "\n",
    "**The main steps for building a Neural Network are:**\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`.\n",
    "\n",
    "we need to first build functions below and then integrate them:\n",
    "   - Sigmoid\n",
    "   - Initialize_with_zeros (to initialize our w and b vectors)\n",
    "   - Propgate\n",
    "   - Optimize (The goal is to learn $w$ and $b$ by minimizing the cost function $J$)\n",
    "   - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6b72d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.array([[1.], [2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb618b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
